#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‹¤ì œ í¬ë¡¤ë§ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ë„¤ì´ë²„ ë¸”ë¡œê·¸ HTMLì—ì„œ ì‹¤ì œ ë²•ë¥  ì½˜í…ì¸ ë§Œ ì¶”ì¶œ
"""
import json
import re
import os
from typing import List, Dict, Any

def clean_naver_blog_content(content: str) -> str:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ HTMLì—ì„œ ì‹¤ì œ ì½˜í…ì¸ ë§Œ ì¶”ì¶œ"""
    if not content:
        return ""
    
    # HTML íƒœê·¸ ì œê±°
    content = re.sub(r'<[^>]+>', '', content)
    
    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ UI ìš”ì†Œ ì œê±°
    ui_patterns = [
        r'ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤\.',
        r'ë‚´ì†Œì‹.*?ë‹«ê¸°',
        r'ì´ì›ƒëª©ë¡.*?ë‹«ê¸°',
        r'í†µê³„.*?ë‹«ê¸°',
        r'í´ë¦½ë§Œë“¤ê¸°.*?ë‹«ê¸°',
        r'ê¸€ì“°ê¸°.*?ë‹«ê¸°',
        r'My Menu ë‹«ê¸°',
        r'ë‚´ ì²´í¬ì¸.*?ë‹«ê¸°',
        r'ìµœê·¼ ë³¸ ê¸€.*?ë‹«ê¸°',
        r'ë‚´ ë™ì˜ìƒ.*?ë‹«ê¸°',
        r'ë‚´ í´ë¦½.*?ë‹«ê¸°',
        r'ë‚´ ìƒí’ˆ ê´€ë¦¬.*?ë‹«ê¸°',
        r'NEW.*?ë‹«ê¸°',
        r'ë§ˆì¼“ í”Œë ˆì´ìŠ¤.*?ë‹«ê¸°',
        r'ì¥ë°”êµ¬ë‹ˆ.*?ë‹«ê¸°',
        r'ë§ˆì¼“ êµ¬ë§¤ë‚´ì—­.*?ë‹«ê¸°',
        r'ë¸”ë¡œê·¸íŒ€ ê³µì‹ë¸”ë¡œê·¸.*?ë‹«ê¸°',
        r'ì´ë‹¬ì˜ ë¸”ë¡œê·¸.*?ë‹«ê¸°',
        r'ê³µì‹ ë¸”ë¡œê·¸.*?ë‹«ê¸°',
        r'ë¸”ë¡œê·¸ ì•±.*?ë‹«ê¸°',
        r'ë¡œê·¸ì¸.*?ë‹«ê¸°',
        r'PCë²„ì „ìœ¼ë¡œ ë³´ê¸°.*?ë‹«ê¸°',
        r'ë¸”ë¡œê·¸ ê³ ê°ì„¼í„°.*?ë‹«ê¸°',
        r'â“’ NAVER Corp\.',
        r'ë³¸ë¬¸ ë°”ë¡œê°€ê¸°.*?ë‹«ê¸°',
        r'ë¸”ë¡œê·¸.*?ë‹«ê¸°',
        r'ì¹´í…Œê³ ë¦¬ ì´ë™.*?ë‹«ê¸°',
        r'ì±„ê¶Œì¶”ì‹¬ì˜ í˜œì•ˆ.*?ë‹«ê¸°',
        r'ê²€ìƒ‰.*?ë‹«ê¸°',
        r'MYë©”ë‰´ ì—´ê¸°.*?ë‹«ê¸°',
        r'ì˜¤ì‹œëŠ”ê¸¸/ì†Œê°œ.*?ë‹«ê¸°',
        r'ë©´ì±…ê³µê³ .*?ë‹«ê¸°',
        r'ì´ì›ƒì¶”ê°€.*?ë‹«ê¸°',
        r'ë³¸ë¬¸ ê¸°íƒ€ ê¸°ëŠ¥.*?ë‹«ê¸°',
        r'ë³¸ë¬¸ í°íŠ¸ í¬ê¸° ì¡°ì •.*?ë‹«ê¸°',
        r'ë³¸ë¬¸ í°íŠ¸ í¬ê¸° ì‘ê²Œ ë³´ê¸°.*?ë‹«ê¸°',
        r'ë³¸ë¬¸ í°íŠ¸ í¬ê¸° í¬ê²Œ ë³´ê¸°.*?ë‹«ê¸°',
        r'ê³µìœ í•˜ê¸°.*?ë‹«ê¸°',
        r'URL ë³µì‚¬.*?ë‹«ê¸°',
        r'ì‹ ê³ í•˜ê¸°.*?ë‹«ê¸°',
        r'ì´ì›ƒì¶”ê°€.*?ë‹«ê¸°',
        r'ë¹„ì¦ˆë‹ˆìŠ¤Â·ê²½ì œ.*?ë‹«ê¸°',
        r'ì´ì›ƒ.*?ëª….*?ë‹«ê¸°',
        r'ì„œì´ˆì—­ 7ë²ˆì¶œêµ¬ ëŒ€í•œë³€í˜¸ì‚¬í˜‘íšŒ ì „ë¬¸ë¡œíŒ 1800-9263.*?ë‹«ê¸°',
        r'ì´ ë¸”ë¡œê·¸.*?ë‹«ê¸°',
        r'ì¹´í…Œê³ ë¦¬ ê¸€.*?ë‹«ê¸°',
        r'ì •ì„±ê³¼ ì‹¤ë ¥ìœ¼ë¡œ ì±„ê¶Œì¶”ì‹¬ì˜ ê²°ê³¼ë¥¼.*?ë‹«ê¸°',
        r'í˜œì•ˆì˜ ë³€í˜¸ì‚¬ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.*?ë‹«ê¸°',
        r'ì±„ê¶Œì¶”ì‹¬ì „ë¬¸ë¡œíŒvsì¼ë°˜ë¡œíŒ.*?ë‹«ê¸°',
        r'ìµœë³‘ì²œ ë³€í˜¸ì‚¬ ì†Œê°œ.*?ë‹«ê¸°',
        r'ìš”ì¦˜ ëœ¨ëŠ” ì‹ ê·œ ê¸€.*?ë‹«ê¸°',
        r'ìš”ì¦˜ì—” ë­ê°€ ì¢‹ì„ì§€.*?ë‹«ê¸°',
        r'ì§„ìƒì†ë‹˜.*?ë‹«ê¸°',
        r'ë³´í†µë‚ .*?ë‹«ê¸°',
        r'ì€ì¹˜.*?ë‹«ê¸°',
        r'ì¥ìˆ˜ëŒ.*?ë‹«ê¸°',
        r'ìƒì€ì´.*?ë‹«ê¸°',
        r'ë¶€í‹°ë¿¡ì´.*?ë‹«ê¸°',
        r'ê³µê°.*?ë‹«ê¸°',
        r'ì¹­ì°¬.*?ë‹«ê¸°',
        r'ê°ì‚¬.*?ë‹«ê¸°',
        r'ì›ƒê¹€.*?ë‹«ê¸°',
        r'ë†€ëŒ.*?ë‹«ê¸°',
        r'ìŠ¬í””.*?ë‹«ê¸°',
        r'ëŒ“ê¸€.*?ë‹«ê¸°',
        r'ì´ì „.*?ë‹«ê¸°',
        r'ë‹¤ìŒ.*?ë‹«ê¸°',
        r'ì·¨ì†Œ.*?ë‹«ê¸°',
        r'ê³µìœ .*?ë‹«ê¸°',
        r'ë‹«ê¸°',
        r'ì´ì›ƒì¶”ê°€í•˜ê³  ìƒˆê¸€ì„ ë°›ì•„ë³´ì„¸ìš”.*?ë‹«ê¸°',
        r'ë‹˜ì„ ì´ì›ƒì¶”ê°€í•˜ê³  ìƒˆê¸€ì„ ë°›ì•„ë³´ì„¸ìš”.*?ë‹«ê¸°',
    ]
    
    for pattern in ui_patterns:
        content = re.sub(pattern, '', content, flags=re.DOTALL)
    
    # JSON ë©”íƒ€ë°ì´í„° ì œê±°
    content = re.sub(r'\{[^}]*"title"[^}]*\}', '', content)
    
    # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
    content = re.sub(r'\s+', ' ', content)
    content = re.sub(r'\n\s*\n', '\n', content)
    
    # ë¹ˆ ì¤„ ì œê±°
    content = re.sub(r'^\s*\n', '', content, flags=re.MULTILINE)
    
    return content.strip()

def extract_legal_content(content: str) -> str:
    """ë²•ë¥  ê´€ë ¨ ì½˜í…ì¸ ë§Œ ì¶”ì¶œ"""
    if not content:
        return ""
    
    # ë²•ë¥  ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ë‹¨ë§Œ ì¶”ì¶œ
    legal_keywords = [
        'ì±„ê¶Œ', 'ì±„ë¬´', 'ì¶”ì‹¬', 'ì§€ê¸‰ëª…ë ¹', 'ë…ì´‰', 'ì§‘í–‰', 'ì†Œì†¡', 'íŒê²°',
        'ë²•ì›', 'ë³€í˜¸ì‚¬', 'ë²•ë¬´ë²•ì¸', 'ë²•ë¥ ', 'ê³„ì•½', 'ì†í•´', 'ë°°ìƒ',
        'ê°•ì œì§‘í–‰', 'ì••ë¥˜', 'ê²½ë§¤', 'ì‹ ìš©ì •ë³´', 'ëŒ€ì—¬ê¸ˆ', 'ë¯¸ìˆ˜ê¸ˆ',
        'ì†Œì•¡ì‚¬ê±´', 'ë¯¼ì‚¬', 'í˜•ì‚¬', 'ê°€ì²˜ë¶„', 'ê°€ì••ë¥˜', 'ê°€ì‚°ê¸ˆ',
        'ì´ì', 'ì—°ì²´', 'ë³€ì œ', 'ìƒí™˜', 'ë‹´ë³´', 'ë³´ì¦', 'ì—°ëŒ€ë³´ì¦'
    ]
    
    # ë¬¸ë‹¨ë³„ë¡œ ë¶„ë¦¬
    paragraphs = content.split('\n')
    legal_paragraphs = []
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if len(paragraph) < 20:  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ ì œì™¸
            continue
            
        # ë²•ë¥  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ë‹¨ë§Œ ì„ íƒ
        if any(keyword in paragraph for keyword in legal_keywords):
            legal_paragraphs.append(paragraph)
    
    return '\n'.join(legal_paragraphs)

def preprocess_crawled_data(input_file: str, output_file: str) -> int:
    """í¬ë¡¤ë§ ë°ì´í„° ì „ì²˜ë¦¬"""
    processed_count = 0
    
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        
        for line_num, line in enumerate(f_in, 1):
            try:
                data = json.loads(line.strip())
                
                # ì›ë³¸ ì½˜í…ì¸  ì¶”ì¶œ
                raw_content = data.get('content', '')
                
                # ë„¤ì´ë²„ ë¸”ë¡œê·¸ UI ì œê±°
                cleaned_content = clean_naver_blog_content(raw_content)
                
                # ë²•ë¥  ì½˜í…ì¸ ë§Œ ì¶”ì¶œ
                legal_content = extract_legal_content(cleaned_content)
                
                # ìµœì†Œ ê¸¸ì´ ì²´í¬ (100ì ì´ìƒ)
                if len(legal_content) < 100:
                    continue
                
                # ì „ì²˜ë¦¬ëœ ë°ì´í„° êµ¬ì¡°
                processed_data = {
                    "id": f"real_doc_{data.get('logno', line_num)}",
                    "text": legal_content,
                    "title": data.get('title', ''),
                    "url": data.get('url', ''),
                    "date": data.get('posted_at', ''),
                    "cat": "ì±„ê¶Œì¶”ì‹¬",
                    "author": "ë²•ë¬´ë²•ì¸ í˜œì•ˆ",
                    "post_type": "ë²•ë¥ ì •ë³´",
                    "original_logno": data.get('logno'),
                    "crawled_at": data.get('crawled_at', ''),
                    "content_length": len(legal_content)
                }
                
                f_out.write(json.dumps(processed_data, ensure_ascii=False) + '\n')
                processed_count += 1
                
                if processed_count % 100 == 0:
                    print(f"ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ë¬¸ì„œ")
                    
            except Exception as e:
                print(f"ë¼ì¸ {line_num} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
    
    return processed_count

def main():
    input_file = "src/data/master/exports/full_crawl_export_2025-10-13_1141.jsonl"
    output_file = "real_legal_corpus.jsonl"
    
    print("ğŸš€ ì‹¤ì œ í¬ë¡¤ë§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    print(f"ì…ë ¥ íŒŒì¼: {input_file}")
    print(f"ì¶œë ¥ íŒŒì¼: {output_file}")
    
    if not os.path.exists(input_file):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        return
    
    processed_count = preprocess_crawled_data(input_file, output_file)
    
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜: {processed_count}ê°œ")
    print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
    
    # ìƒ˜í”Œ í™•ì¸
    if processed_count > 0:
        print("\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° í™•ì¸:")
        with open(output_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 3:  # ì²˜ìŒ 3ê°œë§Œ
                    break
                data = json.loads(line)
                print(f"\n--- ë¬¸ì„œ {i+1} ---")
                print(f"ì œëª©: {data['title']}")
                print(f"ê¸¸ì´: {data['content_length']}ì")
                print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {data['text'][:100]}...")

if __name__ == "__main__":
    main()






