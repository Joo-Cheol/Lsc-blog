#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¦ë¶„ í¬ë¡¤ë§ ì‹œìŠ¤í…œ - ë„¤ì´ë²„ ë¸”ë¡œê·¸ íŠ¹ì • ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
í˜ì´ì§€ë„¤ì´ì…˜ + ì¦ë¶„ ë°©ì‹ìœ¼ë¡œ í¬ë¡¤ë§í•˜ì—¬ ì‹¤í–‰ë§ˆë‹¤ ìŠ¤ëƒ…ìƒ· JSONL ìƒì„±
"""

import re
import json
import time
import random
import argparse
import pathlib
import datetime as dt
import traceback
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException

from state_io import load_last_seen, save_last_seen
from utils_text import clean_text, calculate_content_hash, extract_title_from_content

# ìƒìˆ˜
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
      "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

def setup_driver(headless: bool = True) -> webdriver.Chrome:
    """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
    import tempfile
    import os
    
    opts = Options()
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument(f"--user-agent={UA}")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option('useAutomationExtension', False)
    opts.add_argument("--window-size=1280,900")
    opts.add_argument("--lang=ko-KR")
    
    # ê³ ìœ í•œ ì‚¬ìš©ì ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    temp_dir = tempfile.mkdtemp()
    opts.add_argument(f"--user-data-dir={temp_dir}")
    
    # Chrome ë“œë¼ì´ë²„ ê²½ë¡œ ì§€ì •
    driver_path = "selenium/drivers/chromedriver.exe"
    if os.path.exists(driver_path):
        driver = webdriver.Chrome(service=webdriver.chrome.service.Service(driver_path), options=opts)
    else:
        driver = webdriver.Chrome(options=opts)
    
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver

def fetch_list_page(driver: webdriver.Chrome, blog_id: str, category_no: int, page: int) -> str:
    """ëª©ë¡ í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
    url = (f"https://blog.naver.com/PostList.naver?blogId={blog_id}"
           f"&from=postList&categoryNo={category_no}&currentPage={page}")
    
    driver.get(url)
    time.sleep(2)
    
    # mainFrameìœ¼ë¡œ ì „í™˜ ì‹œë„
    try:
        driver.switch_to.default_content()
        WebDriverWait(driver, 8).until(
            EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe#mainFrame"))
        )
    except TimeoutException:
        pass  # mainFrameì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì§„í–‰
    
    return driver.page_source

def parse_lognos(html: str) -> List[int]:
    """HTMLì—ì„œ logNo ì¶”ì¶œ"""
    soup = BeautifulSoup(html, 'html.parser')
    lognos = set()
    
    # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ logNo ì¶”ì¶œ
    patterns = [
        r'logNo=(\d+)',
        r'data-log-no=["\'](\d+)["\']',
        r'/(\d{6,})(?:\?.*)?$'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, html)
        for match in matches:
            if match.isdigit() and len(match) >= 6:
                lognos.add(int(match))
    
    return sorted(lognos)

def fetch_post_detail(driver: webdriver.Chrome, blog_id: str, logno: int) -> Dict[str, Any]:
    """ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    # ëª¨ë°”ì¼ URLë¡œ ë¨¼ì € ì‹œë„
    mobile_url = f"https://m.blog.naver.com/{blog_id}/{logno}"
    driver.get(mobile_url)
    time.sleep(2)
    
    # ì œëª© ì¶”ì¶œ
    title = "ì œëª© ì—†ìŒ"
    for selector in ["h1.se-title-text", "h1.post-title", "h1.title", ".se-title-text", ".post-title"]:
        try:
            title_element = driver.find_element(By.CSS_SELECTOR, selector)
            title = title_element.text.strip()
            if title:
                break
        except:
            continue
    
    # ë‚´ìš© ì¶”ì¶œ
    content = ""
    for selector in ["div.se-main-container", "#post-view", "div#ct", "div.se_component_wrap"]:
        try:
            content_element = driver.find_element(By.CSS_SELECTOR, selector)
            content = content_element.text.strip()
            if content:
                break
        except:
            continue
    
    # ë‚ ì§œ ì¶”ì¶œ
    posted_at = None
    for selector in ["time[datetime]", "span.se_publishDate", ".se_publishDate", ".post-date"]:
        try:
            date_element = driver.find_element(By.CSS_SELECTOR, selector)
            posted_at = date_element.get_attribute("datetime") or date_element.text.strip()
            if posted_at:
                break
        except:
            continue
    
    # ë‚´ìš© ì •ì œ
    content = clean_text(content)
    if not title or title == "ì œëª© ì—†ìŒ":
        title = extract_title_from_content(content)
    
    return {
        "title": title,
        "content": content,
        "posted_at": posted_at,
        "url": f"https://blog.naver.com/{blog_id}/{logno}"
    }

def run_incremental_crawl(blog_id: str, category_no: int, category_name: str, 
                         outdir: str, max_pages: int = 9999) -> tuple:
    """
    ì¦ë¶„ í¬ë¡¤ë§ ì‹¤í–‰
    
    Returns:
        (output_file_path, new_count, max_logno) íŠœí”Œ
    """
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    run_id = dt.datetime.now().strftime("%Y-%m-%d_%H%M")
    base_dir = pathlib.Path(outdir) / run_id
    base_dir.mkdir(parents=True, exist_ok=True)
    output_file = base_dir / f"posts_{run_id}.jsonl"
    
    # ìƒíƒœ ë¡œë“œ
    last_seen = load_last_seen()
    print(f"ğŸ”„ ì¦ë¶„ í¬ë¡¤ë§ ì‹œì‘ (ë§ˆì§€ë§‰ logno: {last_seen})")
    
    # ë“œë¼ì´ë²„ ì„¤ì •
    driver = setup_driver(headless=True)
    
    try:
        max_logno = last_seen
        new_count = 0
        no_new_pages = 0
        page = 1
        
        with output_file.open("w", encoding="utf-8") as f:
            while page <= max_pages:
                print(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
                
                # ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                html = fetch_list_page(driver, blog_id, category_no, page)
                lognos = parse_lognos(html)
                
                if not lognos:
                    print(f"  âš ï¸ í˜ì´ì§€ {page}ì—ì„œ logNoë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    no_new_pages += 1
                    if no_new_pages >= 3:
                        print("  ğŸ›‘ ì—°ì† 3í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì—†ìŒ - í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                    page += 1
                    continue
                
                print(f"  ğŸ“ {len(lognos)}ê°œ logNo ë°œê²¬")
                
                # ì‹ ê·œ logNoë§Œ í•„í„°ë§
                new_lognos = [ln for ln in lognos if ln > last_seen]
                
                if not new_lognos:
                    print(f"  âš ï¸ í˜ì´ì§€ {page}ì—ì„œ ì‹ ê·œ ê²Œì‹œê¸€ ì—†ìŒ")
                    no_new_pages += 1
                    if no_new_pages >= 2:
                        print("  ğŸ›‘ ì—°ì† 2í˜ì´ì§€ì—ì„œ ì‹ ê·œ ê²Œì‹œê¸€ ì—†ìŒ - í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                else:
                    no_new_pages = 0
                    print(f"  ğŸ†• ì‹ ê·œ {len(new_lognos)}ê°œ ê²Œì‹œê¸€")
                
                # ê° ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                for i, logno in enumerate(new_lognos, 1):
                    print(f"    ğŸ“ [{i}/{len(new_lognos)}] {logno} ì²˜ë¦¬ ì¤‘...")
                    
                    try:
                        detail = fetch_post_detail(driver, blog_id, logno)
                        
                        # ë¬¸ì„œ ìƒì„±
                        doc = {
                            "logno": logno,
                            "url": detail["url"],
                            "title": detail["title"],
                            "category_no": category_no,
                            "category_name": category_name,
                            "posted_at": detail["posted_at"],
                            "content": detail["content"],
                            "crawled_at": dt.datetime.now().astimezone().isoformat(timespec="seconds"),
                            "content_hash": calculate_content_hash(detail["content"])
                        }
                        
                        # JSONL íŒŒì¼ì— ì €ì¥
                        f.write(json.dumps(doc, ensure_ascii=False) + "\n")
                        f.flush()  # ì¦‰ì‹œ ë””ìŠ¤í¬ì— ì“°ê¸°
                        
                        new_count += 1
                        if logno > max_logno:
                            max_logno = logno
                        
                        print(f"      âœ… ì™„ë£Œ: {detail['title'][:50]}...")
                        
                    except Exception as e:
                        print(f"      âŒ ì‹¤íŒ¨: {e}")
                    
                    # ìš”ì²­ ê°„ ì§€ì—°
                    time.sleep(random.uniform(0.5, 1.0))
                
                # í˜ì´ì§€ ê°„ ì§€ì—°
                time.sleep(random.uniform(1.0, 2.0))
                page += 1
        
        # ìƒíƒœ ì €ì¥
        if max_logno > last_seen:
            save_last_seen(max_logno)
            print(f"ğŸ’¾ ìƒíƒœ ì—…ë°ì´íŠ¸: {last_seen} â†’ {max_logno}")
        
        print(f"\nğŸ‰ ì¦ë¶„ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ì‹ ê·œ ìˆ˜ì§‘: {new_count}ê°œ ê²Œì‹œê¸€")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_file}")
        
        return str(output_file), new_count, max_logno
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        print(traceback.format_exc())
        raise
    finally:
        driver.quit()
        print("ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")

def main():
    parser = argparse.ArgumentParser(description="ì¦ë¶„ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬")
    parser.add_argument("--blog-id", required=True, help="ë¸”ë¡œê·¸ ID")
    parser.add_argument("--category-no", type=int, required=True, help="ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸")
    parser.add_argument("--category-name", help="ì¹´í…Œê³ ë¦¬ ì´ë¦„")
    parser.add_argument("--outdir", default="src/data/processed", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--max-pages", type=int, default=9999, help="ìµœëŒ€ í˜ì´ì§€ ìˆ˜")
    parser.add_argument("--run-id", help="ì‹¤í–‰ ID (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ìë™ ìƒì„±)")
    
    args = parser.parse_args()
    
    # ì¹´í…Œê³ ë¦¬ ì´ë¦„ ì„¤ì •
    category_name = args.category_name or f"ì¹´í…Œê³ ë¦¬_{args.category_no}"
    
    try:
        output_file, new_count, max_logno = run_incremental_crawl(
            args.blog_id, 
            args.category_no, 
            category_name,
            args.outdir,
            args.max_pages
        )
        
        print(f"\nğŸ“‹ ì‹¤í–‰ ê²°ê³¼:")
        print(f"  - ì¶œë ¥ íŒŒì¼: {output_file}")
        print(f"  - ì‹ ê·œ ê²Œì‹œê¸€: {new_count}ê°œ")
        print(f"  - ìµœëŒ€ logno: {max_logno}")
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
