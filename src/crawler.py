#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ - ëª¨ë“ /ê°œë³„ ì¹´í…Œê³ ë¦¬ ì§€ì›
ëª¨ë°”ì¼ ëª©ë¡ ìš°ì„  + ë°ìŠ¤í¬í†± ëª©ë¡ í´ë°±(í´ë¦­ ë‚´ë¹„ê²Œì´ì…˜) + ë…¸ì´ì¦ˆ ì œê±°
"""
from __future__ import annotations
import re, json, time, random, argparse, pathlib, datetime as dt, traceback, requests
from requests.adapters import HTTPAdapter, Retry
from bs4 import BeautifulSoup
from tenacity import retry, stop_after_attempt, wait_exponential
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException

# ---------- ìƒìˆ˜/ì •ê·œì‹ ----------
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
      "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

ASYNC_HEADERS = {
    "User-Agent": UA,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9",
    "Referer": "https://blog.naver.com/",
    "Cache-Control": "no-cache",
}

def make_session():
    """ì¬ì‹œë„ì™€ ì—°ê²° í’€ë§ì´ í¬í•¨ëœ requests ì„¸ì…˜ ìƒì„±"""
    s = requests.Session()
    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])
    s.mount("https://", HTTPAdapter(max_retries=retries))
    s.headers.update(ASYNC_HEADERS)
    return s
LOGNO_QS_RE = re.compile(r"logNo=(\d+)")
M_LOGNO_PATH_RE = re.compile(r"/(\d+)(?:\?.*)?$")
M_TOTAL_RE = re.compile(r"(\d{1,3}(?:,\d{3})*)\s*ê°œì˜\s*ê¸€")

# ---------- ìœ í‹¸ ----------
def setup_driver(headless: bool = True):
    opts = Options()
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument(f"--user-agent={UA}")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option('useAutomationExtension', False)
    opts.add_argument(f"--remote-debugging-port={random.randint(9222, 65500)}")
    opts.add_argument("--disable-extensions")
    opts.add_argument("--disable-plugins")
    # opts.add_argument("--disable-images")  # ì¼ë¶€ ìŠ¤í‚¨ì—ì„œ ì´ë¯¸ì§€ ë¡œë”©ì´ ëª©ë¡ DOM íŠ¸ë¦¬ê±°
    opts.add_argument("--window-size=1280,900")
    opts.add_argument("--lang=ko-KR")
    opts.add_experimental_option("prefs", {"intl.accept_languages": "ko-KR,ko"})
    driver = webdriver.Chrome(options=opts)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver

def ensure_dirs(base: pathlib.Path):
    (base / "logs").mkdir(parents=True, exist_ok=True)
    (base / "by_category").mkdir(parents=True, exist_ok=True)

def load_state(path: pathlib.Path) -> dict:
    return json.loads(path.read_text(encoding="utf-8")) if path.exists() else {}

def save_state(path: pathlib.Path, state: dict):
    path.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def sanitize_filename(s: str) -> str:
    return re.sub(r"[\\/:*?\"<>|]", "_", s).strip()

def switch_to_mainframe_if_present(driver, timeout=8) -> bool:
    driver.switch_to.default_content()
    try:
        WebDriverWait(driver, timeout).until(
            EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe#mainFrame"))
        )
        return True
    except TimeoutException:
        return False

def ensure_in_mainframe(driver, timeout=8) -> bool:
    driver.switch_to.default_content()
    try:
        WebDriverWait(driver, timeout).until(
            EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, "iframe#mainFrame"))
        )
        return True
    except TimeoutException:
        return False

def _bs(html: str) -> BeautifulSoup:
    for parser in ("lxml", "html5lib", "html.parser"):
        try:
            return BeautifulSoup(html, parser)
        except Exception:
            continue
    return BeautifulSoup(html, "html.parser")

def clean_text(html: str) -> tuple[str, str]:
    soup = _bs(html)
    for tag in soup(["script", "style"]):
        tag.decompose()
    return soup.get_text("\n", strip=True), str(soup)

# ---------- ì¹´í…Œê³ ë¦¬ íƒìƒ‰(ë°ìŠ¤í¬í†±) ----------
def discover_categories(driver, blog_id: str) -> list[dict]:
    url = f"https://blog.naver.com/PostList.naver?blogId={blog_id}&from=postList&categoryNo=0&currentPage=1"
    driver.get(url)
    switch_to_mainframe_if_present(driver, timeout=10)

    cats: dict[int, dict] = {}
    anchors = driver.find_elements(By.CSS_SELECTOR, "a[href*='categoryNo=']")
    for a in anchors:
        href = a.get_attribute("href") or ""
        m = re.search(r"categoryNo=(\d+)", href)
        if not m:
            continue
        cat_no = int(m.group(1))
        if cat_no == 0:
            continue
        name = (a.text or "").strip()
        name_clean = re.sub(r"[()\uFF08\uFF09]\s*\d[\d,]*\s*[()\uFF08\uFF09]", "", name).strip()
        mcount = re.search(r"[()\uFF08\uFF09]\s*(\d[\d,]*)\s*[()\uFF08\uFF09]", name)
        count_hint = int(mcount.group(1).replace(",", "")) if mcount else None
        # ì¹´í…Œê³ ë¦¬ 8ë²ˆ "ì‰¬ëŠ”ì‹œê°„" ì œì™¸
        if cat_no == 8 and ("ì‰¬ëŠ”ì‹œê°„" in name_clean or "íœ´ì‹" in name_clean):
            continue
        cats.setdefault(cat_no, {"cat_no": cat_no, "name": name_clean, "count_hint": count_hint})

    out = sorted(cats.values(), key=lambda d: d["cat_no"])
    if not out:
        print("âš ï¸ ì¹´í…Œê³ ë¦¬ íƒìƒ‰ ì‹¤íŒ¨(ìŠ¤í‚¨ ì°¨ì´).")
    return out

# ---------- ëª©ë¡ ì§„ì… ----------
def goto_mobile_list(driver, blog_id: str, cat_no: int, page: int, wait_seconds: int = 15):
    url = (f"https://m.blog.naver.com/PostList.naver?blogId={blog_id}"
           f"&from=postList&categoryNo={cat_no}&currentPage={page}")
    driver.get(url)
    WebDriverWait(driver, wait_seconds).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a")))
    _gentle_scroll(driver, steps=4, pause=0.35)

def _frame_src_current_page(driver) -> int | None:
    """mainFrame srcì˜ currentPage ê°’ì„ ì½ìŒ(ì—†ìœ¼ë©´ None). í˜¸ì¶œ ì „ mainFrame ì•ˆì´ì–´ì•¼ í•¨."""
    try:
        frame = driver.find_element(By.CSS_SELECTOR, "iframe#mainFrame")
        src = frame.get_attribute("src") or ""
    except Exception:
        try:
            # ì´ë¯¸ í”„ë ˆì„ ë‚´ë¶€ë¼ë©´ document.URL ì‚¬ìš©
            src = driver.execute_script("return document.location.href;") or ""
        except Exception:
            return None
    m = re.search(r"currentPage=(\d+)", src)
    return int(m.group(1)) if m else None

def get_category_total_async(blog_id: str, cat_no: int, count_per_page: int = 30) -> tuple[int,int,int,str]:
    """async ì—”ë“œí¬ì¸íŠ¸ë¡œ ì •í™•í•œ ì´ ê¸€ ìˆ˜ì™€ ë§ˆì§€ë§‰ í˜ì´ì§€ ê³„ì‚°"""
    total = 0
    page = 1
    session = make_session()
    while True:
        url = ("https://blog.naver.com/PostTitleListAsync.naver"
               f"?blogId={blog_id}&from=postList&categoryNo={cat_no}"
               f"&currentPage={page}&countPerPage={count_per_page}")
        try:
            r = session.get(url, timeout=8)
            if not r.ok or not r.text:
                break
            lognos = set(re.findall(r"logNo=(\d{6,})", r.text))
            if not lognos:
                break
            total += len(lognos)
            page += 1
            # ë°©ì–´: ê³¼ë„í•œ í˜ì´ì§€ ìˆœíšŒ ì œí•œ
            if page > 2000: break
            # ìš”ì²­ ê°„ ì§€ì—° (ì•ˆí‹°ë´‡)
            time.sleep(random.uniform(0.1, 0.3))
        except Exception:
            break
    last_page = max(1, (total - 1)//count_per_page + 1) if total else 1
    return total, count_per_page, last_page, "async"

def fetch_lognos_async(blog_id: str, cat_no: int, page: int) -> list[str]:
    """
    ë„¤ì´ë²„ ë‚´ë¶€ ëª©ë¡ ë¹„ë™ê¸° ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì§ì ‘ ì¡°íšŒí•´ì„œ logNo ë¦¬ìŠ¤íŠ¸ë¥¼ ë½‘ëŠ”ë‹¤.
    1) ë°ìŠ¤í¬í†±: PostTitleListAsync.naver
    2) ëª¨ë°”ì¼:  CategoryPostListAsync.naver (í´ë°±)
    """
    out = set()
    session = make_session()

    # ë°ìŠ¤í¬í†± async
    desktop_url = (
        "https://blog.naver.com/PostTitleListAsync.naver"
        f"?blogId={blog_id}&from=postList&categoryNo={cat_no}"
        f"&currentPage={page}&countPerPage=30"
    )
    try:
        r = session.get(desktop_url, timeout=8)
        if r.ok and r.text:
            out.update(re.findall(r"logNo=(\d{6,})", r.text))
            out.update(re.findall(r"data-log-no=[\"'](\d+)[\"']", r.text))
    except Exception:
        pass

    # ëª¨ë°”ì¼ async (í´ë°±)
    if not out:
        mobile_url = (
            "https://m.blog.naver.com/CategoryPostListAsync.naver"
            f"?blogId={blog_id}&categoryNo={cat_no}"
            f"&currentPage={page}&countPerPage=30"
        )
        try:
            r = session.get(mobile_url, timeout=8)
            if r.ok and r.text:
                out.update(re.findall(r"/%s/(\d{6,})" % re.escape(blog_id), r.text))
                out.update(re.findall(r"logNo=(\d{6,})", r.text))
                out.update(re.findall(r"data-log-no=[\"'](\d+)[\"']", r.text))
        except Exception:
            pass

    return sorted(out)

def _gentle_scroll(driver, steps=3, pause=0.4):
    """ì§€ì—° ë¡œë”© ëŒ€ë¹„ ì²œì²œíˆ ìŠ¤í¬ë¡¤"""
    for _ in range(steps):
        driver.execute_script("window.scrollBy(0, document.body.scrollHeight/2);")
        time.sleep(pause)
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(pause)

def goto_desktop_list(driver, blog_id: str, cat_no: int, page: int, wait_seconds: int = 15):
    url = (f"https://blog.naver.com/PostList.naver?blogId={blog_id}"
           f"&from=postList&categoryNo={cat_no}&currentPage={page}")
    driver.get(url)

    in_frame = ensure_in_mainframe(driver, timeout=wait_seconds)
    WebDriverWait(driver, wait_seconds).until(
        EC.presence_of_any_elements_located((By.CSS_SELECTOR, "a[href*='logNo='], [data-log-no]"))
        if in_frame else
        EC.presence_of_any_elements_located((By.CSS_SELECTOR, "iframe#mainFrame, a[href*='logNo='], [data-log-no]"))
    )
    _gentle_scroll(driver, steps=4, pause=0.35)

# ---------- ëª©ë¡ì—ì„œ logNo ì¶”ì¶œ ----------
def collect_lognos_on_mobile_page(driver, blog_id: str) -> list[str]:
    out = set()
    for a in driver.find_elements(By.CSS_SELECTOR, "a[href]"):
        href = a.get_attribute("href") or ""
        m = re.search(r"/%s/(\d{6,})" % re.escape(blog_id), href)
        if m: out.add(m.group(1))
    if not out:
        html = driver.page_source
        out.update(re.findall(r"/%s/(\d{6,})" % re.escape(blog_id), html))
    return sorted(out)

def collect_lognos_on_desktop_page(driver, blog_id: str) -> list[str]:
    out = set()

    # DOM ê¸°ë°˜
    for a in driver.find_elements(By.CSS_SELECTOR, "a[href*='logNo=']"):
        href = a.get_attribute("href") or ""
        m = LOGNO_QS_RE.search(href)
        if m: out.add(m.group(1))
    for node in driver.find_elements(By.CSS_SELECTOR, "[data-log-no]"):
        ln = (node.get_attribute("data-log-no") or "").strip()
        if ln.isdigit(): out.add(ln)

    # í´ë°±: HTML ì „ì²´ì—ì„œ ì •ê·œì‹
    if not out:
        html = driver.page_source
        out.update(re.findall(r"logNo=(\d{6,})", html))

    return sorted(out)

# ---------- ë³¸ë¬¸ ìˆ˜ì§‘ ----------
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=8))
def fetch_post_html_mobile(driver, blog_id: str, logno: str) -> str:
    url = f"https://m.blog.naver.com/{blog_id}/{logno}"
    driver.get(url)
    for sel in ["div.se-main-container", "#post-view", "div#ct", "div.se_component_wrap"]:
        try:
            WebDriverWait(driver, 8).until(EC.presence_of_element_located((By.CSS_SELECTOR, sel)))
            return driver.page_source
        except TimeoutException:
            continue
    return driver.page_source

@retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=1, max=6))
def fetch_post_html_desktop(driver, blog_id: str, logno: str) -> str:
    url = f"https://blog.naver.com/{blog_id}/{logno}"
    driver.get(url)
    switch_to_mainframe_if_present(driver, timeout=12)
    for sel in ["div.se-main-container", "div#postViewArea", "[id^='post-view']",
                "div.post_ct", "div.post-view", "div.contents"]:
        try:
            WebDriverWait(driver, 8).until(EC.presence_of_element_located((By.CSS_SELECTOR, sel)))
            return driver.page_source
        except TimeoutException:
            continue
    return driver.page_source

def fetch_post_html_mobile_first(driver, blog_id: str, logno: str) -> str:
    try:
        return fetch_post_html_mobile(driver, blog_id, logno)
    except Exception:
        return fetch_post_html_desktop(driver, blog_id, logno)

def extract_metadata(soup: BeautifulSoup) -> dict:
    md = {"title": None, "published_at": None, "author": None, "images": [], "tags": []}
    # ì œëª© í›„ë³´ ì¶”ê°€ (ë” ë‹¤ì–‘í•œ ìŠ¤í‚¨ ì§€ì›)
    for sel in ["meta[property='og:title']", "h1.se-title-text", "h1.post-title", "h1.title", 
                ".se-title-text", ".post-title", "h3.se_textarea", ".pcol1 .htitle", 
                "h2#title_1", "h2#logNo", "title"]:
        el = soup.select_one(sel)
        if el:
            md["title"] = el.get("content", None) or el.get_text(strip=True)
            if md["title"]: break
    # ë‚ ì§œ í›„ë³´ ì¶”ê°€ (ë” ë‹¤ì–‘í•œ ìŠ¤í‚¨ ì§€ì›)
    for sel in ["time[datetime]", "span.se_publishDate", "span#se_publishDate", 
                ".se_publishDate", ".post-date", ".date", "[class*='date']",
                "meta[property='article:published_time']", "meta[name='date']", ".publish-date"]:
        el = soup.select_one(sel)
        if el:
            md["published_at"] = el.get("datetime") or el.get("content") or el.get_text(strip=True)
            if md["published_at"]: break
    # ì‘ì„±ì í›„ë³´ ì¶”ê°€
    for sel in [".nick", ".bloger", "[class*='author']", "span.se_author", "span.author", 
                ".author", ".post-author", "meta[property='article:author']", ".blog-author", ".writer"]:
        el = soup.select_one(sel)
        if el and el.get_text(strip=True):
            md["author"] = el.get_text(strip=True)
            if md["author"]: break
    # ì´ë¯¸ì§€ ìˆ˜ì§‘ (ë” í¬ê´„ì )
    for img in soup.find_all("img"):
        src = img.get("src") or img.get("data-src")
        if src and src.startswith("http") and ("blog.naver.com" in src or "mblogthumb-phinf.pstatic.net" in src):
            md["images"].append(src)
    # íƒœê·¸ ìˆ˜ì§‘ (ë” í¬ê´„ì )
    for sel in [".tag", ".tags a", "[class*='tag'] a", "a[href*='tag=']", ".post-tag", ".category-tag"]:
        for t in soup.select(sel):
            val = t.get_text(strip=True)
            if val and val not in md["tags"] and len(val) < 50:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸
                md["tags"].append(val)
    return md

# ---------- ì´í•©/ë§ˆì§€ë§‰ í˜ì´ì§€ ê³„ì‚° ----------
def parse_total_from_screen(driver) -> int:
    try:
        inner = driver.find_element(By.CSS_SELECTOR, "body").get_attribute("innerText") or ""
    except Exception:
        return 0
    m = M_TOTAL_RE.search(inner)
    return int(m.group(1).replace(",", "")) if m else 0

def get_category_total(driver, blog_id: str, cat_no: int, count_hint: int | None) -> tuple[int, int, int, str]:
    """
    (total, page_size, last_page, method)
    1) async ì—”ë“œí¬ì¸íŠ¸ ìš°ì„  (ì •í™•ë„ ìµœê³ )
    2) sidebar-hint í´ë°±
    3) ì‹¤íŒ¨ ì‹œ: ë°ìŠ¤í¬í†± 1í˜ì´ì§€ ê°•ì œ ì§„ì… í›„ page_size ì¸¡ì • â†’ 9999 ì í”„ ë°©ì‹
    4) ê·¸ë˜ë„ ì‹¤íŒ¨ì‹œ ëª¨ë°”ì¼ë¡œ ë™ì¼
    """
    # 0) async ì—”ë“œí¬ì¸íŠ¸ë¡œ ì •í™•í•œ ì´ ê°œìˆ˜ ê³„ì‚° (ìµœìš°ì„ )
    try:
        total, page_size, last_page, method = get_category_total_async(blog_id, cat_no)
        if total > 0:
            return total, page_size, last_page, method
    except Exception:
        pass

    # 1) ì‚¬ì´ë“œë°” íŒíŠ¸ê°€ ìˆìœ¼ë©´ í´ë°±
    if count_hint and count_hint > 0:
        # ì‹¤ì œ 1í˜ì´ì§€ì˜ page_sizeë¥¼ ë°ìŠ¤í¬í†±ì—ì„œ ì¸¡ì •
        try:
            goto_desktop_list(driver, blog_id, cat_no, 1)
            page1 = collect_lognos_on_desktop_page(driver, blog_id)
            page_size = len(page1) or 50
        except Exception:
            page_size = 50  # ì•ˆì „ ê¸°ë³¸ê°’
        last_page = max(1, (count_hint - 1) // page_size + 1)
        return count_hint, page_size, last_page, "sidebar-hint"

    # 2) ì‚¬ì´ë“œë°” íŒíŠ¸ê°€ ì—†ìœ¼ë©´ ë°ìŠ¤í¬í†± í˜ì´ì§€ ì›Œí¬
    try:
        goto_desktop_list(driver, blog_id, cat_no, 1)
        p1 = collect_lognos_on_desktop_page(driver, blog_id)
        page_size = len(p1) or 50
        # ë§ˆì§€ë§‰ ë¸”ë¡ ì¶”ì •
        goto_desktop_list(driver, blog_id, cat_no, 9999)
        # í˜„ì¬ mainFrame URLì—ì„œ currentPageë¥¼ ì½ì–´ ìµœëŒ“ê°’ìœ¼ë¡œ ì‚¬ìš© (9999 â†’ ìë™ í´ë¨í”„)
        driver.switch_to.default_content()
        ensure_in_mainframe(driver, timeout=8)  # í”„ë ˆì„ ë³´ì¥
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "iframe#mainFrame"))
        )
        last_src = driver.find_element(By.CSS_SELECTOR, "iframe#mainFrame").get_attribute("src") or ""
        m = re.search(r"currentPage=(\d+)", last_src)
        max_page = int(m.group(1)) if m else 1
        goto_desktop_list(driver, blog_id, cat_no, max_page)
        last_count = len(collect_lognos_on_desktop_page(driver, blog_id)) or page_size
        total = (max_page - 1) * page_size + last_count
        return total, page_size, max_page, "desktop-walk"
    except Exception:
        pass

    # 3) ìµœí›„ì—” ëª¨ë°”ì¼
    try:
        goto_mobile_list(driver, blog_id, cat_no, 1)
        p1 = collect_lognos_on_mobile_page(driver, blog_id)
        page_size = len(p1) or 10
        # ëª¨ë°”ì¼ì€ ì•µì»¤ê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 9999 ì í”„ í›„ body í…ìŠ¤íŠ¸ì˜ "Nê°œì˜ ê¸€" ë¬¸êµ¬ë¥¼ ì‹œë„
        goto_mobile_list(driver, blog_id, cat_no, 9999)
        total = parse_total_from_screen(driver)
        if not total:
            total = page_size  # ìµœì†Œê°’
        last_page = max(1, (total - 1) // page_size + 1)
        return total, page_size, last_page, "mobile-fallback"
    except Exception:
        return 0, 10, 1, "unknown"

# ---------- ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ----------
def crawl_category(driver, blog_id: str, cat_no: int, cat_name: str,
                   start_page: int, max_pages: int,
                   state: dict, out_jsonl: pathlib.Path, sidebar_counts: dict[int, int] | None = None) -> list[dict]:
    print(f"\nğŸ—‚ï¸ ì¹´í…Œê³ ë¦¬[{cat_no}] {cat_name} ì‹œì‘")

    count_hint = (sidebar_counts or {}).get(cat_no)
    total, page_size, last_page, method = get_category_total(driver, blog_id, cat_no, count_hint)
    print(f"ğŸ§® ì´ ê¸€ ìˆ˜ ì¶”ì •: {total} (page_sizeâ‰ˆ{page_size}, last_pageâ‰ˆ{last_page}, via {method})")

    if not total:
        print("  âš ï¸ ì´í•© ê³„ì‚° ì‹¤íŒ¨ â†’ 'max-pages' í•œë„ê¹Œì§€ ì „ì§„í•©ë‹ˆë‹¤.")
        effective_max_pages = max_pages
    else:
        effective_max_pages = max(0, min(max_pages, last_page - start_page + 1))
    print(f"ğŸ“Š ì‹¤ì œ ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜: {effective_max_pages}")

    state.setdefault("categories", {})
    prev = state["categories"].get(str(cat_no), {})
    last_log_no = prev.get("last_log_no")

    # ê°™ì€ ì‹¤í–‰ ë‚´/íŒŒì¼ ë‚´ ì¤‘ë³µ ë°©ì§€
    seen_already = set()
    if out_jsonl.exists():
        with out_jsonl.open(encoding="utf-8") as f:
            for line in f:
                try:
                    j = json.loads(line)
                    if j.get("source", {}).get("category_no") == cat_no and j.get("post_no"):
                        seen_already.add(j["post_no"])
                except Exception:
                    continue

    results, seen_session = [], set()
    page = start_page
    stop_at_page = start_page + effective_max_pages - 1
    empty_streak = 0
    consecutive_empty = 0
    prev_sig = None
    repeat_hits = 0

    while effective_max_pages == 0 or page <= stop_at_page:
        print(f"ğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")

        # 0) ë¹„ë™ê¸° ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¨¼ì € ì‹œë„
        lognos = []
        try:
            lognos = fetch_lognos_async(blog_id, cat_no, page)
            if lognos:
                print(f"  âš¡ async ëª©ë¡ {len(lognos)}ê±´")
        except Exception:
            lognos = []

        # 1) (ë¹„ì—ˆìœ¼ë©´) ëª¨ë°”ì¼ ëª©ë¡ ì‹œë„
        if not lognos:
            try:
                goto_mobile_list(driver, blog_id, cat_no, page)
                lognos = collect_lognos_on_mobile_page(driver, blog_id)
            except Exception:
                lognos = []

        # 2) (ê·¸ë˜ë„ ë¹„ë©´) ë°ìŠ¤í¬í†± ëª©ë¡ í´ë°±
        if not lognos:
            print("  ğŸ” ëª¨ë°”ì¼ ëª©ë¡ ë¹„ì–´ìˆìŒ â†’ ë°ìŠ¤í¬í†± ëª©ë¡ í´ë°±")
            try:
                goto_desktop_list(driver, blog_id, cat_no, page)
                ensure_in_mainframe(driver, timeout=6)
                lognos = collect_lognos_on_desktop_page(driver, blog_id)
                print(f"  ğŸ“Œ got {len(lognos)} lognos (desktop)")
            except Exception:
                lognos = []

        if not lognos:
            # ì•ˆí‹°ë´‡/ë¹ˆí™”ë©´ ë””ë²„ê·¸ ê°€ë“œ
            in_frame = ensure_in_mainframe(driver, timeout=6)
            try:
                cur_url = driver.execute_script("return document.location.href;")
            except Exception:
                cur_url = "(no frame url)"
            try:
                body_txt = driver.find_element(By.TAG_NAME, "body").get_attribute("innerText")[:500].replace("\n"," ")
            except Exception:
                body_txt = "(no body)"
            print(f"  ğŸªµ empty-list peek @ {cur_url} :: {body_txt[:220]} ...")
                
            print("  â— ëª©ë¡ ë¹„ì–´ìˆìŒ â†’ ë‹¤ìŒ í˜ì´ì§€")
            page += 1
            empty_streak += 1
            if empty_streak >= 3:   # ì—°ì† ë¹ˆ í˜ì´ì§€ 3íšŒë©´ ì¢…ë£Œ(ìŠ¤í‚¨/í˜ì´ì§• ë³´í˜¸)
                print("  ğŸ›‘ ì—°ì† 3í˜ì´ì§€ ë¹„ì–´ìˆìŒ â†’ ì¹´í…Œê³ ë¦¬ ì¢…ë£Œ")
                break
            continue
        empty_streak = 0

        # ê°™ì€ í˜ì´ì§€ ë°˜ë³µ ê°ì§€
        sig = ",".join(lognos[:10])  # ì• 10ê°œë§Œ ì„œëª…ì²˜ëŸ¼ ì‚¬ìš©
        if sig == prev_sig:
            repeat_hits += 1
        else:
            repeat_hits = 0
        prev_sig = sig

        if repeat_hits >= 2:
            print("  ğŸ›‘ ê°™ì€ ëª©ë¡ì´ 3í˜ì´ì§€ ì—°ì† ë°˜ë³µë¨ â†’ ì¹´í…Œê³ ë¦¬ ì¢…ë£Œ(í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ ì¶”ì •)")
            break

        print("  ğŸ” ìƒ˜í”Œ logno:", ", ".join(lognos[:5]))

        filtered = []
        for ln in lognos:
            if last_log_no and int(ln) <= int(last_log_no):
                continue
            if ln in seen_session or ln in seen_already:
                continue
            seen_session.add(ln)
            filtered.append(ln)

        print(f"  ğŸ“ ì‹ ê·œ {len(filtered)}ê°œ")
        
        # ì¤‘ë‹¨ ì¡°ê±´ ê°•í™” (ì¦ë¶„ ìˆ˜ì§‘)
        if not filtered and last_log_no:
            consecutive_empty += 1
            if consecutive_empty >= 2:
                print("  â›³ ì‹ ê·œ ì—†ìŒ 2í˜ì´ì§€ ì—°ì† â†’ ì¹´í…Œê³ ë¦¬ ì¢…ë£Œ")
                break
        else:
            consecutive_empty = 0
            
        for ln in sorted(filtered):
            print(f"    ğŸ“ {ln} ìˆ˜ì§‘ ì¤‘...")
            try:
                html = fetch_post_html_mobile_first(driver, blog_id, ln)
                text, content_html = clean_text(html)
                soup = _bs(html)
                meta = extract_metadata(soup)
                now = dt.datetime.now().astimezone().isoformat(timespec="seconds")
                rec = {
                    "post_no": ln,
                    "title": meta["title"] or f"ê²Œì‹œê¸€ {ln}",
                    "category": cat_name,
                    "author": meta["author"],
                    "url": f"https://blog.naver.com/{blog_id}/{ln}",
                    "published_at": meta["published_at"],
                    "crawled_at": now,
                    "content_text": text,
                    "content_html": content_html,
                    "images": meta["images"],
                    "tags": meta["tags"],
                    "source": {"blog_id": blog_id, "category_no": cat_no, "page": page}
                }
                results.append(rec)
                with out_jsonl.open("a", encoding="utf-8") as f:
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                print(f"      âœ… ì™„ë£Œ: {(rec['title'] or '')[:50]}...")
            except Exception as e:
                print(f"      âŒ ì‹¤íŒ¨: {e}")
            time.sleep(random.uniform(0.4, 1.0))

        time.sleep(random.uniform(0.4, 0.8))
        page += 1

    # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼
    cat_file = out_jsonl.parent / "by_category" / f"{cat_no}_{sanitize_filename(cat_name or str(cat_no))}.json"
    cat_file.write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding="utf-8")

    # ìƒíƒœ ê°±ì‹ 
    new_max = max([int(r["post_no"]) for r in results], default=prev.get("last_log_no", 0) or 0)
    state["categories"][str(cat_no)] = {
        "last_log_no": new_max,
        "last_page": last_page,
        "updated_at": dt.datetime.now().isoformat(timespec="seconds"),
        "name": cat_name
    }
    return results

# ---------- ë©”ì¸ ----------
def main():
    ap = argparse.ArgumentParser(description="ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ (ëª¨ë°”ì¼ ëª©ë¡ ìš°ì„  + ë°ìŠ¤í¬í†± í´ë¦­ í´ë°±)")
    ap.add_argument("--blog-id", required=True)
    ap.add_argument("--category-no", type=int, default=6)
    ap.add_argument("--all-categories", action="store_true")
    ap.add_argument("--start-page", type=int, default=1)
    ap.add_argument("--max-pages", type=int, default=9999)
    ap.add_argument("--outdir", default="src/data/processed")
    ap.add_argument("--run-id", default=None)
    ap.add_argument("--resume", action="store_true")
    ap.add_argument("--since-logno", type=int, default=None)
    ap.add_argument("--headless", action="store_true")
    args = ap.parse_args()

    run_id = args.run_id or dt.datetime.now().strftime("%Y-%m-%d_%H%M")
    base = pathlib.Path(args.outdir) / run_id
    ensure_dirs(base)

    state_path = pathlib.Path(args.outdir) / "state.json"
    state = load_state(state_path) if args.resume else {}
    if "categories" not in state:
        state["categories"] = {}

    driver = setup_driver(headless=args.headless)
    out_jsonl = base / "posts_all.jsonl"

    try:
        if args.all_categories:
            cats = discover_categories(driver, args.blog_id)
            if not cats:
                print("â— ì¹´í…Œê³ ë¦¬ ìë™ íƒìƒ‰ ì‹¤íŒ¨. --category-no ë¡œ ê°œë³„ ì‹¤í–‰í•˜ì„¸ìš”.")
                return
            target_cats = cats
            sidebar_counts = { c["cat_no"]: c.get("count_hint") for c in cats }
            print("ğŸ§­ ìˆœíšŒ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬:", ", ".join(f"{c['cat_no']}:{c['name']}" for c in cats))
        else:
            target_cats = [{"cat_no": args.category_no, "name": f"category_{args.category_no}", "count_hint": None}]
            sidebar_counts = {}

        grand_total = 0
        for cat in target_cats:
            cat_no = int(cat["cat_no"])
            cat_name = cat.get("name") or f"category_{cat_no}"
            if args.since_logno is not None:
                state["categories"][str(cat_no)] = {"last_log_no": args.since_logno}
            rows = crawl_category(driver, args.blog_id, cat_no, cat_name,
                                  args.start_page, args.max_pages, state, out_jsonl, sidebar_counts)
            grand_total += len(rows)
            state.update({"blog_id": args.blog_id, "run_id": run_id})
            save_state(state_path, state)

        print(f"\nğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {grand_total}ê°œ ê²Œì‹œê¸€")
        print(f"ğŸ’¾ ì €ì¥ ë””ë ‰í† ë¦¬: {base}")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print(traceback.format_exc())
    finally:
        driver.quit()
        print("ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")

if __name__ == "__main__":
    main()
    