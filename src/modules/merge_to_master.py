#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìŠ¤ëƒ…ìƒ· JSONL â†’ SQLite ë³‘í•© (ì¤‘ë³µ ì°¨ë‹¨ + ì‹ ê·œë§Œ ì¶”ì¶œ)
"""

import sqlite3
import json
import os
import argparse
from pathlib import Path
from utils_text import calculate_content_hash

DB_PATH = "src/data/master/posts.sqlite"

def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    
    cur.executescript("""
    PRAGMA journal_mode=WAL;
    PRAGMA synchronous=NORMAL;
    
    CREATE TABLE IF NOT EXISTS posts (
        logno INTEGER PRIMARY KEY,
        url TEXT,
        title TEXT,
        category_no INTEGER,
        posted_at TEXT,
        content TEXT,
        content_hash TEXT,
        crawled_at TEXT,
        created_at TEXT DEFAULT (datetime('now')),
        updated_at TEXT DEFAULT (datetime('now'))
    );
    
    CREATE INDEX IF NOT EXISTS idx_posts_category ON posts(category_no);
    CREATE INDEX IF NOT EXISTS idx_posts_posted_at ON posts(posted_at);
    CREATE INDEX IF NOT EXISTS idx_posts_crawled_at ON posts(crawled_at);
    """)
    
    conn.commit()
    conn.close()
    print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: {DB_PATH}")

def upsert_jsonl(jsonl_path: str) -> list:
    """
    JSONL íŒŒì¼ì„ SQLiteì— ë³‘í•©í•˜ê³  ì‹ ê·œ ì‚½ì…ëœ logno ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Args:
        jsonl_path: JSONL íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì‹ ê·œ ì‚½ì…ëœ logno ë¦¬ìŠ¤íŠ¸
    """
    if not os.path.exists(jsonl_path):
        raise FileNotFoundError(f"JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")
    
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    
    inserted_lognos = []
    updated_count = 0
    new_count = 0
    
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            try:
                data = json.loads(line.strip())
                
                # post_noë¥¼ lognoë¡œ ë§¤í•‘
                logno = int(data.get("post_no", data.get("logno", 0)))
                content_hash = calculate_content_hash(data.get("content_text", data.get("content", "")))
                
                # ê¸°ì¡´ ë ˆì½”ë“œ í™•ì¸
                cur.execute("SELECT content_hash FROM posts WHERE logno = ?", (logno,))
                existing = cur.fetchone()
                
                # UPSERT ì‹¤í–‰
                cur.execute("""
                INSERT INTO posts (logno, url, title, category_no, posted_at, content, content_hash, crawled_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(logno) DO UPDATE SET
                    url = excluded.url,
                    title = excluded.title,
                    category_no = excluded.category_no,
                    posted_at = excluded.posted_at,
                    content = CASE 
                        WHEN posts.content_hash != excluded.content_hash 
                        THEN excluded.content 
                        ELSE posts.content 
                    END,
                    content_hash = CASE 
                        WHEN posts.content_hash != excluded.content_hash 
                        THEN excluded.content_hash 
                        ELSE posts.content_hash 
                    END,
                    crawled_at = excluded.crawled_at,
                    updated_at = datetime('now')
                """, (
                    logno,
                    data.get("url", ""),
                    data.get("title", ""),
                    int(data.get("category_no", 0)),
                    data.get("published_at", data.get("posted_at", "")),
                    data.get("content_text", data.get("content", "")),
                    content_hash,
                    data.get("crawled_at", "")
                ))
                
                if existing is None:
                    # ì‹ ê·œ ì‚½ì…
                    inserted_lognos.append(logno)
                    new_count += 1
                elif existing[0] != content_hash:
                    # ë‚´ìš© ë³€ê²½ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                    updated_count += 1
                
            except (json.JSONDecodeError, KeyError, ValueError) as e:
                print(f"âš ï¸ ë¼ì¸ {line_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
    
    conn.commit()
    conn.close()
    
    print(f"ğŸ“Š ë³‘í•© ì™„ë£Œ:")
    print(f"  - ì‹ ê·œ ì‚½ì…: {new_count}ê°œ")
    print(f"  - ë‚´ìš© ì—…ë°ì´íŠ¸: {updated_count}ê°œ")
    print(f"  - ì´ ì²˜ë¦¬: {len(inserted_lognos)}ê°œ")
    
    return inserted_lognos

def export_new_posts(inserted_lognos: list, run_id: str) -> str:
    """ì‹ ê·œ ì‚½ì…ëœ í¬ìŠ¤íŠ¸ë“¤ì„ ë³„ë„ JSONLë¡œ ë‚´ë³´ë‚´ê¸°"""
    if not inserted_lognos:
        print("ğŸ“ ì‹ ê·œ í¬ìŠ¤íŠ¸ê°€ ì—†ì–´ ë‚´ë³´ë‚´ê¸°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return ""
    
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    
    # ì‹ ê·œ í¬ìŠ¤íŠ¸ ì¡°íšŒ
    placeholders = ",".join("?" * len(inserted_lognos))
    cur.execute(f"""
    SELECT logno, url, title, category_no, posted_at, content, content_hash, crawled_at
    FROM posts 
    WHERE logno IN ({placeholders})
    ORDER BY logno
    """, inserted_lognos)
    
    posts = cur.fetchall()
    conn.close()
    
    # ë‚´ë³´ë‚´ê¸° íŒŒì¼ ê²½ë¡œ
    export_dir = "src/data/master/exports"
    os.makedirs(export_dir, exist_ok=True)
    export_path = os.path.join(export_dir, f"new_for_index_{run_id}.jsonl")
    
    # JSONL íŒŒì¼ ìƒì„±
    with open(export_path, "w", encoding="utf-8") as f:
        for post in posts:
            data = {
                "logno": post[0],
                "url": post[1],
                "title": post[2],
                "category_no": post[3],
                "posted_at": post[4],
                "content": post[5],
                "content_hash": post[6],
                "crawled_at": post[7]
            }
            f.write(json.dumps(data, ensure_ascii=False) + "\n")
    
    print(f"ğŸ“¤ ì‹ ê·œ í¬ìŠ¤íŠ¸ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {export_path}")
    return export_path

def get_database_stats() -> dict:
    """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    
    # ì „ì²´ í¬ìŠ¤íŠ¸ ìˆ˜
    cur.execute("SELECT COUNT(*) FROM posts")
    total_posts = cur.fetchone()[0]
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    cur.execute("""
    SELECT category_no, COUNT(*) 
    FROM posts 
    GROUP BY category_no 
    ORDER BY category_no
    """)
    category_stats = dict(cur.fetchall())
    
    # ìµœì‹ /ìµœì˜¤ë˜ëœ í¬ìŠ¤íŠ¸
    cur.execute("SELECT MIN(crawled_at), MAX(crawled_at) FROM posts")
    min_date, max_date = cur.fetchone()
    
    conn.close()
    
    return {
        "total_posts": total_posts,
        "category_stats": category_stats,
        "date_range": {"min": min_date, "max": max_date}
    }

def main():
    parser = argparse.ArgumentParser(description="JSONL ìŠ¤ëƒ…ìƒ·ì„ SQLiteì— ë³‘í•©")
    parser.add_argument("--input", required=True, help="ì…ë ¥ JSONL íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--run-id", help="ì‹¤í–‰ ID (ë‚´ë³´ë‚´ê¸° íŒŒì¼ëª…ìš©)")
    parser.add_argument("--stats", action="store_true", help="í†µê³„ ì •ë³´ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    init_database()
    
    # ë³‘í•© ì‹¤í–‰
    inserted_lognos = upsert_jsonl(args.input)
    
    # ì‹ ê·œ í¬ìŠ¤íŠ¸ ë‚´ë³´ë‚´ê¸°
    if inserted_lognos and args.run_id:
        export_new_posts(inserted_lognos, args.run_id)
    
    # í†µê³„ ì¶œë ¥
    if args.stats:
        stats = get_database_stats()
        print(f"\nğŸ“ˆ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
        print(f"  - ì´ í¬ìŠ¤íŠ¸: {stats['total_posts']}ê°œ")
        print(f"  - ì¹´í…Œê³ ë¦¬ë³„: {stats['category_stats']}")
        print(f"  - ë‚ ì§œ ë²”ìœ„: {stats['date_range']['min']} ~ {stats['date_range']['max']}")

if __name__ == "__main__":
    main()
