#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í”„ë¡œë•ì…˜ RAG ê¸°ë°˜ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì‹œìŠ¤í…œ
ì¹´í…Œê³ ë¦¬ ì¤‘ì‹¬ ë¦¬íŠ¸ë¦¬ë²Œ + ì¡°ê±´ë¶€ ìŠ¤í•„ì˜¤ë²„ + ìŠ¤íƒ€ì¼ ë±…í¬
"""

import os
import json
import sqlite3
import argparse
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import re

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("âš ï¸ ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install chromadb")

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸ Gemini APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-generativeai")

class ProductionRAGGenerator:
    def __init__(self, 
                 db_path: str = "src/data/master/posts.sqlite",
                 chroma_path: str = "src/data/indexes/chroma",
                 knowledge_collection: str = "naver_blog_debt_collection",
                 gemini_api_key: str = None):
        """
        í”„ë¡œë•ì…˜ RAG ìƒì„±ê¸° ì´ˆê¸°í™”
        
        Args:
            db_path: SQLite ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
            chroma_path: ChromaDB ì €ì¥ ê²½ë¡œ
            knowledge_collection: ì§€ì‹ ì»¬ë ‰ì…˜ ì´ë¦„
            gemini_api_key: Gemini API í‚¤
        """
        self.db_path = db_path
        self.chroma_path = chroma_path
        self.knowledge_collection = knowledge_collection
        self.gemini_api_key = gemini_api_key or os.getenv("GEMINI_API_KEY")
        
        if not self.gemini_api_key:
            raise ValueError("Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ GEMINI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì§ì ‘ ì „ë‹¬í•˜ì„¸ìš”.")
        
        if GEMINI_AVAILABLE:
            genai.configure(api_key=self.gemini_api_key)
            self.model = genai.GenerativeModel('gemini-2.0-flash')
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if CHROMADB_AVAILABLE:
            self.client = chromadb.PersistentClient(
                path=chroma_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            self._init_collections()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        if not os.path.exists(db_path):
            raise FileNotFoundError(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")
    
    def _init_collections(self):
        """ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            self.knowledge_col = self.client.get_collection(self.knowledge_collection)
            print(f"ğŸ“š ê¸°ì¡´ ì§€ì‹ ì»¬ë ‰ì…˜ ë¡œë“œ: {self.knowledge_collection}")
        except Exception:
            try:
                import chromadb.utils.embedding_functions as embedding_functions
                embedding_function = embedding_functions.DefaultEmbeddingFunction()
                self.knowledge_col = self.client.create_collection(
                    name=self.knowledge_collection,
                    embedding_function=embedding_function,
                    metadata={"description": "ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì „ì²´ ì§€ì‹ ì €ì¥ì†Œ"}
                )
                print(f"ğŸ“š ìƒˆ ì§€ì‹ ì»¬ë ‰ì…˜ ìƒì„±: {self.knowledge_collection}")
            except Exception:
                self.knowledge_col = self.client.create_collection(
                    name=self.knowledge_collection,
                    metadata={"description": "ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì „ì²´ ì§€ì‹ ì €ì¥ì†Œ"}
                )
                print(f"ğŸ“š ìƒˆ ì§€ì‹ ì»¬ë ‰ì…˜ ìƒì„±: {self.knowledge_collection} (ì„ë² ë”© í•¨ìˆ˜ ì—†ìŒ)")
    
    def get_style_collection(self, category_no: int) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ìŠ¤íƒ€ì¼ ì»¬ë ‰ì…˜ ì´ë¦„ ë°˜í™˜"""
        return f"style_cat_{category_no}"
    
    def create_style_collection(self, category_no: int):
        """ì¹´í…Œê³ ë¦¬ë³„ ìŠ¤íƒ€ì¼ ì»¬ë ‰ì…˜ ìƒì„±"""
        collection_name = self.get_style_collection(category_no)
        try:
            return self.client.get_collection(collection_name)
        except Exception:
            try:
                import chromadb.utils.embedding_functions as embedding_functions
                embedding_function = embedding_functions.DefaultEmbeddingFunction()
                return self.client.create_collection(
                    name=collection_name,
                    embedding_function=embedding_function,
                    metadata={"description": f"ì¹´í…Œê³ ë¦¬ {category_no} ìŠ¤íƒ€ì¼ ë±…í¬"}
                )
            except Exception:
                return self.client.create_collection(
                    name=collection_name,
                    metadata={"description": f"ì¹´í…Œê³ ë¦¬ {category_no} ìŠ¤íƒ€ì¼ ë±…í¬"}
                )
    
    def chunk_text(self, text: str, max_tokens: int = 1200, overlap: int = 200) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if not text:
            return [""]
        
        # ê°„ë‹¨í•œ ê¸€ììˆ˜ ê¸°ë°˜ ì²­í‚¹ (í† í° ëŒ€ì‹  ê¸€ììˆ˜ ì‚¬ìš©)
        max_chars = max_tokens * 2  # ëŒ€ëµì ì¸ ë³€í™˜
        step = max_chars - overlap
        
        chunks = []
        for i in range(0, len(text), step):
            chunk = text[i:i + max_chars]
            if chunk.strip():
                chunks.append(chunk.strip())
        
        return chunks if chunks else [""]
    
    def upsert_to_chroma(self, docs: List[Dict], run_id: str, source_file: str) -> int:
        """ë¬¸ì„œë“¤ì„ ChromaDBì— upsert"""
        if not CHROMADB_AVAILABLE:
            print("âŒ ChromaDBê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
            return 0
        
        ids, texts, metas = [], [], []
        
        for doc in docs:
            chunks = self.chunk_text(doc.get("content", ""), max_tokens=1200, overlap=200)
            
            for i, chunk in enumerate(chunks):
                chunk_id = f'{doc["logno"]}:{i:03d}'
                ids.append(chunk_id)
                texts.append(chunk)
                metas.append({
                    "logno": int(doc["logno"]),
                    "chunk_idx": i,
                    "category_no": int(doc.get("category_no", 0)),
                    "posted_at": doc.get("posted_at", ""),
                    "title": doc.get("title", ""),
                    "url": doc.get("url", ""),
                    "run_id": run_id,
                    "source_file": source_file,
                    "content_hash": doc.get("content_hash", "")
                })
        
        # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ upsert
        batch_size = 100
        total_upserted = 0
        
        for i in range(0, len(ids), batch_size):
            batch_ids = ids[i:i + batch_size]
            batch_texts = texts[i:i + batch_size]
            batch_metas = metas[i:i + batch_size]
            
            try:
                self.knowledge_col.upsert(
                    ids=batch_ids,
                    documents=batch_texts,
                    metadatas=batch_metas
                )
                total_upserted += len(batch_ids)
                print(f"ğŸ“¤ ë°°ì¹˜ {i//batch_size + 1}: {len(batch_ids)}ê°œ ì²­í¬ upsert ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i//batch_size + 1} upsert ì‹¤íŒ¨: {str(e)}")
                # ê°œë³„ ì²­í¬ë¡œ ì‹œë„
                for j, (chunk_id, chunk_text, chunk_meta) in enumerate(zip(batch_ids, batch_texts, batch_metas)):
                    try:
                        self.knowledge_col.upsert(
                            ids=[chunk_id],
                            documents=[chunk_text],
                            metadatas=[chunk_meta]
                        )
                        total_upserted += 1
                    except Exception as e2:
                        print(f"âŒ ê°œë³„ ì²­í¬ {chunk_id} upsert ì‹¤íŒ¨: {str(e2)}")
        
        return total_upserted
    
    def retrieve_with_spillover(self, query: str, category_no: int, 
                               spillover_categories: List[int] = None,
                               top_k: int = 8, spillover_k: int = 2) -> Dict[str, Any]:
        """
        ì¹´í…Œê³ ë¦¬ ì¤‘ì‹¬ ë¦¬íŠ¸ë¦¬ë²Œ + ì¡°ê±´ë¶€ ìŠ¤í•„ì˜¤ë²„
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            category_no: ë©”ì¸ ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸
            spillover_categories: ìŠ¤í•„ì˜¤ë²„ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ë“¤
            top_k: ë©”ì¸ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            spillover_k: ìŠ¤í•„ì˜¤ë²„ ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not CHROMADB_AVAILABLE:
            return {"documents": [], "metadatas": [], "distances": []}
        
        # 1ì°¨ ê²€ìƒ‰: ì¹´í…Œê³ ë¦¬ ì ê¸ˆ
        try:
            main_results = self.knowledge_col.query(
                query_texts=[query],
                n_results=top_k,
                where={"category_no": int(category_no)},
                include=["documents", "metadatas", "distances"]
            )
        except Exception as e:
            print(f"âŒ ë©”ì¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return {"documents": [], "metadatas": [], "distances": []}
        
        # í’ˆì§ˆ í‰ê°€
        need_spillover = self._evaluate_quality(main_results)
        
        spillover_results = {"documents": [], "metadatas": []}
        
        if need_spillover and spillover_categories:
            try:
                spillover_results = self.knowledge_col.query(
                    query_texts=[query],
                    n_results=spillover_k,
                    where={"category_no": {"$in": [int(cat) for cat in spillover_categories]}},
                    include=["documents", "metadatas"]
                )
                print(f"ğŸ”„ ìŠ¤í•„ì˜¤ë²„ í™œì„±í™”: {spillover_categories}ì—ì„œ {len(spillover_results.get('documents', [[]])[0])}ê°œ ì¶”ê°€")
            except Exception as e:
                print(f"âŒ ìŠ¤í•„ì˜¤ë²„ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        
        # ê²°ê³¼ ë³‘í•©
        return self._merge_results(main_results, spillover_results)
    
    def _evaluate_quality(self, results: Dict[str, Any]) -> bool:
        """ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€"""
        if not results.get("distances") or not results["distances"][0]:
            return True
        
        distances = results["distances"][0][:3]  # ìƒìœ„ 3ê°œë§Œ í‰ê°€
        similarities = [1 - d for d in distances]
        avg_similarity = sum(similarities) / len(similarities)
        
        # í‰ê·  ìœ ì‚¬ë„ê°€ 0.72 ë¯¸ë§Œì´ë©´ ìŠ¤í•„ì˜¤ë²„ í•„ìš”
        return avg_similarity < 0.72
    
    def _merge_results(self, main: Dict[str, Any], spillover: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ ë³‘í•©"""
        merged = {
            "documents": main.get("documents", [[]])[0] + spillover.get("documents", [[]])[0],
            "metadatas": main.get("metadatas", [[]])[0] + spillover.get("metadatas", [[]])[0],
            "distances": main.get("distances", [[]])[0] + [0.5] * len(spillover.get("documents", [[]])[0])
        }
        
        # ì¤‘ë³µ ì œê±° (ë™ì¼ lognoì—ì„œ ìµœëŒ€ 2ê°œë§Œ)
        seen_lognos = {}
        filtered_docs, filtered_metas, filtered_dists = [], [], []
        
        for doc, meta, dist in zip(merged["documents"], merged["metadatas"], merged["distances"]):
            logno = meta.get("logno", 0)
            if logno not in seen_lognos:
                seen_lognos[logno] = 0
            
            if seen_lognos[logno] < 2:  # ë™ì¼ lognoì—ì„œ ìµœëŒ€ 2ê°œ
                filtered_docs.append(doc)
                filtered_metas.append(meta)
                filtered_dists.append(dist)
                seen_lognos[logno] += 1
        
        return {
            "documents": filtered_docs,
            "metadatas": filtered_metas,
            "distances": filtered_dists
        }
    
    def get_style_context(self, query: str, category_no: int, top_m: int = 3) -> str:
        """ìŠ¤íƒ€ì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not CHROMADB_AVAILABLE:
            return ""
        
        try:
            style_col = self.create_style_collection(category_no)
            results = style_col.query(
                query_texts=[query],
                n_results=top_m,
                include=["documents", "metadatas"]
            )
            
            if not results.get("documents") or not results["documents"][0]:
                return ""
            
            # ìŠ¤íƒ€ì¼ ìš”ì†Œë§Œ ì¶”ì¶œ (í—¤ë”, ë¬¸ì¥ íŒ¨í„´ ë“±)
            style_snippets = []
            for doc in results["documents"][0]:
                # ê°„ë‹¨í•œ ìŠ¤íƒ€ì¼ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¡œì§ í•„ìš”)
                lines = doc.split('\n')
                for line in lines:
                    if any(keyword in line for keyword in ['##', '**', 'â€¢', '1.', '2.', '3.']):
                        style_snippets.append(line.strip())
                        if len(style_snippets) >= 5:  # ìµœëŒ€ 5ê°œ ìŠ¤ë‹ˆí«
                            break
            
            return '\n'.join(style_snippets[:5])
        except Exception as e:
            print(f"âŒ ìŠ¤íƒ€ì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return ""
    
    def build_context(self, query: str, category_no: int, 
                     spillover_categories: List[int] = None,
                     max_tokens: int = 2800) -> str:
        """ì „ì²´ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        # ì§€ì‹ ê²€ìƒ‰
        knowledge_results = self.retrieve_with_spillover(
            query, category_no, spillover_categories
        )
        
        # ìŠ¤íƒ€ì¼ ì»¨í…ìŠ¤íŠ¸
        style_context = self.get_style_context(query, category_no)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        
        # ì§€ì‹ ì»¨í…ìŠ¤íŠ¸ (80-90%)
        if knowledge_results.get("documents"):
            for i, (doc, meta) in enumerate(zip(knowledge_results["documents"], knowledge_results["metadatas"])):
                context_parts.append(f"""
ì œëª©: {meta.get('title', 'N/A')}
URL: {meta.get('url', 'N/A')}
ì‘ì„±ì¼: {meta.get('posted_at', 'N/A')}
ë‚´ìš©: {doc[:800]}...
---
""")
        
        # ìŠ¤íƒ€ì¼ ì»¨í…ìŠ¤íŠ¸ (10-20%)
        if style_context:
            context_parts.append(f"""
ìŠ¤íƒ€ì¼ ì°¸ê³ :
{style_context}
---
""")
        
        full_context = '\n'.join(context_parts)
        
        # í† í° ìˆ˜ ì œí•œ (ëŒ€ëµì ì¸ ê¸€ììˆ˜ ê¸°ë°˜)
        if len(full_context) > max_tokens * 2:
            full_context = full_context[:max_tokens * 2] + "..."
        
        return full_context
    
    def generate_article(self, topic: str, category_no: int, 
                        style: str = "professional",
                        spillover_categories: List[int] = None) -> str:
        """
        ì£¼ì œì— ëŒ€í•œ ì „ë¬¸ ê¸€ ìƒì„±
        
        Args:
            topic: ê¸€ ì£¼ì œ
            category_no: ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸
            style: ê¸€ ìŠ¤íƒ€ì¼
            spillover_categories: ìŠ¤í•„ì˜¤ë²„ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ë“¤
            
        Returns:
            ìƒì„±ëœ ê¸€
        """
        if not GEMINI_AVAILABLE:
            return "âŒ Gemini APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        print(f"ğŸ” '{topic}' ê´€ë ¨ í¬ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ (ì¹´í…Œê³ ë¦¬: {category_no})...")
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.build_context(topic, category_no, spillover_categories)
        
        if not context:
            return "âŒ ê´€ë ¨ í¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"ğŸ“š ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ ({len(context)}ì)")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        style_instructions = {
            "professional": "ì „ë¬¸ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.",
            "casual": "ì¹œê·¼í•˜ê³  ì½ê¸° ì‰¬ìš´ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.",
            "academic": "í•™ìˆ ì ì´ê³  ì •í™•í•œ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
        }
        
        prompt = f"""
ë‹¤ìŒì€ ì±„ê¶Œì¶”ì‹¬ ê´€ë ¨ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë“¤ì…ë‹ˆë‹¤:

{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ "{topic}"ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. {style_instructions.get(style, style_instructions['professional'])}
2. ì»¨í…ìŠ¤íŠ¸(ì¹´í…Œê³ ë¦¬ {category_no})ì—ì„œ ì œê³µëœ ê·¼ê±°ë§Œ ì‚¬ìš©í•´ ì‘ì„±í•˜ì„¸ìš”.
3. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì  ê°€ì´ë“œë¡œ í•œì •í•˜ê³  ì¶”ì •í•˜ê±°ë‚˜ ë‹¨ì •í•˜ì§€ ë§ˆì„¸ìš”.
4. ë²•ë¥ ëª…/ì ˆì°¨/ê¸°í•œ/ê¸°ê´€ëª…/ìˆ˜ì¹˜ëŠ” ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ê²½ìš°ì—ë§Œ ì¸ìš©í•˜ê³ , ì¶œì²˜/ë‚ ì§œê°€ ë¶ˆëª…í™•í•˜ë©´ 'ì‚¬ë¡€ì— ë”°ë¼ ë‹¤ë¦„'ì„ ëª…ì‹œí•˜ì„¸ìš”.
5. ë¬¸ì²´ëŠ” ìŠ¤íƒ€ì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë”°ë¥´ë˜, ì‚¬ì‹¤ê³¼ ì¶©ëŒ ì‹œ ì‚¬ì‹¤ì„ ìš°ì„ í•˜ì„¸ìš”.
6. ì‹¤ì œ ì‚¬ë¡€ì™€ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
7. ë…ìì—ê²Œ ì‹¤ìš©ì ì¸ ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”
8. 1000-1500ì ì •ë„ì˜ ë¶„ëŸ‰ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
9. ì œëª©, ë³¸ë¬¸, ê²°ë¡ ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”

ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
"""
        
        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"âŒ ê¸€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def get_database_stats(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # ì´ í¬ìŠ¤íŠ¸ ìˆ˜
        cur.execute("SELECT COUNT(*) FROM posts")
        total_posts = cur.fetchone()[0]
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        cur.execute("SELECT category_no, COUNT(*) FROM posts GROUP BY category_no")
        category_stats = dict(cur.fetchall())
        
        # ë‚ ì§œ ë²”ìœ„
        cur.execute("SELECT MIN(posted_at), MAX(posted_at) FROM posts")
        date_range = cur.fetchone()
        
        conn.close()
        
        return {
            "total_posts": total_posts,
            "category_stats": category_stats,
            "date_range": date_range
        }
    
    def get_chroma_stats(self) -> Dict[str, Any]:
        """ChromaDB í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not CHROMADB_AVAILABLE:
            return {"error": "ChromaDB ì‚¬ìš© ë¶ˆê°€"}
        
        try:
            count = self.knowledge_col.count()
            return {
                "knowledge_collection_count": count,
                "collection_name": self.knowledge_collection
            }
        except Exception as e:
            return {"error": str(e)}

def main():
    parser = argparse.ArgumentParser(description="í”„ë¡œë•ì…˜ RAG ê¸°ë°˜ ë¸”ë¡œê·¸ ê¸€ ìƒì„±ê¸°")
    parser.add_argument("--topic", help="ê¸€ ì£¼ì œ")
    parser.add_argument("--category", type=int, help="ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸")
    parser.add_argument("--style", choices=["professional", "casual", "academic"], 
                       default="professional", help="ê¸€ ìŠ¤íƒ€ì¼")
    parser.add_argument("--spillover", nargs="*", type=int, help="ìŠ¤í•„ì˜¤ë²„ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ë“¤")
    parser.add_argument("--db-path", default="src/data/master/posts.sqlite", 
                       help="SQLite ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ")
    parser.add_argument("--chroma-path", default="src/data/indexes/chroma",
                       help="ChromaDB ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--collection", default="naver_blog_all",
                       help="ì§€ì‹ ì»¬ë ‰ì…˜ ì´ë¦„")
    parser.add_argument("--stats", action="store_true", help="í†µê³„ ì¶œë ¥")
    parser.add_argument("--vectorize", help="ë²¡í„°í™”í•  JSONL íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--run-id", help="ì‹¤í–‰ ID (ë²¡í„°í™” ì‹œ í•„ìš”)")
    parser.add_argument("--source-file", help="ì†ŒìŠ¤ íŒŒì¼ ê²½ë¡œ (ë²¡í„°í™” ì‹œ í•„ìš”)")
    
    args = parser.parse_args()
    
    try:
        # RAG ìƒì„±ê¸° ì´ˆê¸°í™”
        rag = ProductionRAGGenerator(
            db_path=args.db_path,
            chroma_path=args.chroma_path,
            knowledge_collection=args.collection
        )
        
        # ë²¡í„°í™” ëª¨ë“œ
        if args.vectorize:
            if not args.run_id or not args.source_file:
                print("âŒ ë²¡í„°í™” ëª¨ë“œì—ì„œëŠ” --run-idì™€ --source-fileì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return
            
            print(f"ğŸ“„ ë²¡í„°í™” ì‹œì‘: {args.vectorize}")
            
            # JSONL íŒŒì¼ ë¡œë“œ
            docs = []
            with open(args.vectorize, "r", encoding="utf-8") as f:
                for line in f:
                    docs.append(json.loads(line.strip()))
            
            print(f"ğŸ“š {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
            
            # ë²¡í„°í™”
            upserted_count = rag.upsert_to_chroma(docs, args.run_id, args.source_file)
            print(f"âœ… ë²¡í„°í™” ì™„ë£Œ: {upserted_count}ê°œ ì²­í¬ upsert")
            return
        
        # í†µê³„ ì¶œë ¥
        if args.stats:
            db_stats = rag.get_database_stats()
            chroma_stats = rag.get_chroma_stats()
            
            print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
            print(f"  - ì´ í¬ìŠ¤íŠ¸: {db_stats['total_posts']}ê°œ")
            print(f"  - ì¹´í…Œê³ ë¦¬ë³„: {db_stats['category_stats']}")
            print(f"  - ë‚ ì§œ ë²”ìœ„: {db_stats['date_range'][0]} ~ {db_stats['date_range'][1]}")
            
            print("\nğŸ“Š ChromaDB í†µê³„:")
            if "error" in chroma_stats:
                print(f"  - ì˜¤ë¥˜: {chroma_stats['error']}")
            else:
                print(f"  - ì§€ì‹ ì»¬ë ‰ì…˜: {chroma_stats['collection_name']}")
                print(f"  - ì´ ë²¡í„° ìˆ˜: {chroma_stats['knowledge_collection_count']}ê°œ")
            print()
        
        # ê¸€ ìƒì„±
        if not args.topic or not args.category:
            print("âŒ ê¸€ ìƒì„± ëª¨ë“œì—ì„œëŠ” --topicê³¼ --categoryê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        print(f"ğŸ¯ ì£¼ì œ: {args.topic}")
        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {args.category}")
        print(f"ğŸ“ ìŠ¤íƒ€ì¼: {args.style}")
        if args.spillover:
            print(f"ğŸ”„ ìŠ¤í•„ì˜¤ë²„: {args.spillover}")
        print("=" * 50)
        
        article = rag.generate_article(
            topic=args.topic,
            category_no=args.category,
            style=args.style,
            spillover_categories=args.spillover
        )
        
        print(article)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
