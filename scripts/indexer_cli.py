#!/usr/bin/env python3
"""
RAG ì¸ë±ìŠ¤ êµ¬ì¶• CLI ë„êµ¬
ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""
import argparse
import json
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.search.embedding import get_embedder
    from src.search.store import get_chroma
    from src.search.indexer import chunk_text
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    print("í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

def main():
    ap = argparse.ArgumentParser(description="RAG ì¸ë±ìŠ¤ êµ¬ì¶• ë„êµ¬")
    ap.add_argument("--in", dest="inp", required=True, help="ì…ë ¥ JSONL íŒŒì¼ ê²½ë¡œ")
    ap.add_argument("--out", dest="out", default=".chroma", help="Chroma ì €ì¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸: .chroma)")
    ap.add_argument("--model", default=os.getenv("EMBED_MODEL", "intfloat/multilingual-e5-base"), help="ì„ë² ë”© ëª¨ë¸")
    ap.add_argument("--chunk", type=int, default=400, help="ì²­í¬ í¬ê¸° (ê¸°ë³¸: 400)")
    ap.add_argument("--overlap", type=int, default=60, help="ì²­í¬ ê²¹ì¹¨ (ê¸°ë³¸: 60)")
    ap.add_argument("--normalize", action="store_true", help="ì„ë² ë”© ì •ê·œí™”")
    args = ap.parse_args()

    print(f"ğŸš€ ì¸ë±ì‹± ì‹œì‘: {args.inp} â†’ {args.out}")
    print(f"ğŸ“Š ì„¤ì •: ëª¨ë¸={args.model}, ì²­í¬={args.chunk}, ê²¹ì¹¨={args.overlap}")

    # ì„ë² ë” ì´ˆê¸°í™”
    print("ğŸ”§ ì„ë² ë” ì´ˆê¸°í™” ì¤‘...")
    embedder = get_embedder(model_name=args.model, normalize=args.normalize)
    
    # Chroma DB ì´ˆê¸°í™”
    print("ğŸ—„ï¸ Chroma DB ì´ˆê¸°í™” ì¤‘...")
    db = get_chroma(persist_dir=args.out, space="cosine")

    # ë°ì´í„° ë¡œë“œ ë° ì²­í‚¹
    print("ğŸ“– ë°ì´í„° ë¡œë“œ ë° ì²­í‚¹ ì¤‘...")
    docs, metas, ids = [], [], []
    
    input_file = Path(args.inp)
    if not input_file.exists():
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.inp}")
        sys.exit(1)
    
    for i, line in enumerate(input_file.read_text(encoding="utf-8").splitlines()):
        if not line.strip():
            continue
            
        try:
            row = json.loads(line)
            chunks = chunk_text(row["content"], chunk_size=args.chunk, overlap=args.overlap)
            
            for j, chunk in enumerate(chunks):
                docs.append(chunk)
                metas.append({
                    k: row.get(k) for k in ["cat", "date", "title", "url", "author", "post_type"]
                })
                ids.append(f"{i}-{j}")
                
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜ (ë¼ì¸ {i+1}): {e}")
            continue

    if not docs:
        print("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    print(f"ğŸ“ ì´ {len(docs)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")

    # ì„ë² ë”© ìƒì„±
    print("ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘...")
    embs = embedder.encode_passages(docs)
    
    # Chroma DBì— ì €ì¥
    print("ğŸ’¾ Chroma DBì— ì €ì¥ ì¤‘...")
    db.add_texts(docs, metadatas=metas, ids=ids, embeddings=embs)
    
    print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {len(docs)}ê°œ ì²­í¬ â†’ {args.out}")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {Path(args.out).absolute()}")

if __name__ == "__main__":
    main()





